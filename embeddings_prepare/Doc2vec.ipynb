{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Doc2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1vXQO89InOB"
      },
      "source": [
        "import gensim\n",
        "import codecs\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from collections import OrderedDict\n",
        "import multiprocessing\n",
        "\n",
        "\n",
        "from random import shuffle\n",
        "import datetime\n",
        "\n",
        "cores = multiprocessing.cpu_count()\n",
        "\n",
        "VOC_SIZE = 200"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-ceHAmdO5FG",
        "outputId": "3231add0-aee5-4914-8d6a-e55b68a13689"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPTb4b60Oy-P"
      },
      "source": [
        "# Arxiv dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofk81fTasEVg"
      },
      "source": [
        "### clean triplets (without path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HhQvegumngz"
      },
      "source": [
        "clean_arxiv_triplets = []\n",
        "with codecs.open(\"data/arxiv_triplets.txt\") as fin:\n",
        "    for line in fin:\n",
        "        idx1, idx2, idx3 = (path.split('/')[-1] for path in line.split(' '))\n",
        "        clean_arxiv_triplets.append(f'{idx1} {idx2} {idx3}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smq80aALpZdE"
      },
      "source": [
        "with codecs.open(\"data/clean_arxiv_triplets.txt\", \"w\") as fin:\n",
        "    for triplet in clean_arxiv_triplets:\n",
        "        fin.write(triplet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "dVJCx4WKInN-"
      },
      "source": [
        "### Doc2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlqF2H3at3b7"
      },
      "source": [
        "def docs_iterator(filename, start_from=0):\n",
        "    with codecs.open(filename, encoding='utf-8') as fin:\n",
        "        for line_no, line in enumerate(fin):\n",
        "            tokens = gensim.utils.to_unicode(line).split()\n",
        "            yield TaggedDocument(tokens[start_from:], [line_no])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XycCnqenOy-S"
      },
      "source": [
        "class MyCorpus_train:\n",
        "    def __iter__(self, ):\n",
        "        with codecs.open(train_filename, encoding='utf-8') as fin:\n",
        "            for line_no, line in enumerate(fin):\n",
        "                tokens = gensim.utils.to_unicode(line).split()\n",
        "                yield TaggedDocument(tokens[0:], [line_no])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFTYbLg-9Sug"
      },
      "source": [
        "### train vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9rALn-iJJYh"
      },
      "source": [
        "train_filename = 'data/arxiv/arxiv_plain.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X40XBBzcInOD"
      },
      "source": [
        "dbow = Doc2Vec(dm=0, vector_size=VOC_SIZE, negative=5, hs=0, min_count=0, workers=8)\n",
        "dbow.build_vocab(docs_iterator(train_filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFL4TphMUBnx"
      },
      "source": [
        "dbow.save('train_doc2vec/train0.doc2vec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75j4b8lYOy-U"
      },
      "source": [
        "dbow = Doc2Vec.load('train_doc2vec/train5.doc2vec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBChG4T6Oy-U"
      },
      "source": [
        "# если начинать с некоторого шага\n",
        "alpha, min_alpha, passes = (0.025, 0.0001, 15)\n",
        "alpha_delta = (alpha - min_alpha) / passes\n",
        "alpha -= 5 * alpha_delta\n",
        "\n",
        "for epoch in range(5, passes):\n",
        "    dbow.alpha, dbow.min_alpha = alpha, alpha       \n",
        "    dbow.train(MyCorpus_train(), total_examples=dbow.corpus_count, epochs=10)        \n",
        "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
        "    dbow.save(f'train_doc2vec/train{epoch + 1}.doc2vec')  \n",
        "    print('saved step')\n",
        "    alpha -= alpha_delta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP6NWKNzInOE"
      },
      "source": [
        "alpha, min_alpha, passes = (0.025, 0.0001, 15)\n",
        "alpha_delta = (alpha - min_alpha) / passes\n",
        "\n",
        "for epoch in range(passes):\n",
        "    dbow.alpha, dbow.min_alpha = alpha, alpha       \n",
        "    dbow.train(MyCorpus_train(), total_examples=dbow.corpus_count, epochs=10)        \n",
        "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
        "    dbow.save(f'train_doc2vec/train{epoch + 1}.doc2vec')  \n",
        "    print('saved step')\n",
        "    alpha -= alpha_delta       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeafYMtPInOF"
      },
      "source": [
        "dbow.save('train_doc2vec/train_total.doc2vec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRP-YjUZGljf"
      },
      "source": [
        "### test vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRe8ubL-Oy-W"
      },
      "source": [
        "test_filename = 'data/arxiv/test_arxiv_plain.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS51HQ8GOy-W"
      },
      "source": [
        "class MyCorpus_test:\n",
        "    def __iter__(self, ):\n",
        "        with codecs.open(test_filename, encoding='utf-8') as fin:\n",
        "            for line_no, line in enumerate(fin):\n",
        "                tokens = gensim.utils.to_unicode(line).split()\n",
        "                yield TaggedDocument(tokens[0:], [line_no])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVVVCz2aGS3T"
      },
      "source": [
        "test_id2tag = {}\n",
        "with codecs.open(test_filename, encoding='utf-8') as fin:\n",
        "    for line_no, line in enumerate(fin):\n",
        "        test_id2tag[line.split()[0]] = line_no"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R2I-gGSGwP4"
      },
      "source": [
        "infer_steps = 5\n",
        "infer_alpha = 0.1\n",
        "test_vectors = [dbow.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) \n",
        "                for doc in docs_iterator('test_arxiv_plain.txt', start_from=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTxRv5ESG1tj"
      },
      "source": [
        "with codecs.open('data/arxiv/embeddings_doc2vec.txt', 'w') as fin:\n",
        "    for id, tag in test_id2tag.items():\n",
        "        line = 'd-' + id + ' ' + ' '.join(map(str, test_vectors[tag])) + '\\n'\n",
        "        fin.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wmRDwFOOy-X"
      },
      "source": [
        "# MIND dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjs66GbjOy-Y"
      },
      "source": [
        "# берем уже обученную модель Doc2Vec на базе arxiv, потому что база супер большая - 25 ГБ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__A-t2DrOy-Y"
      },
      "source": [
        "#local\n",
        "#dbow = Doc2Vec.load('Doc2Vec_model_trained_arxiv/train_total.doc2vec')\n",
        "\n",
        "#colab\n",
        "dbow = Doc2Vec.load('/content/drive/MyDrive/diploma/train_doc2vec/train_total.doc2vec')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg97PCiWQDtd"
      },
      "source": [
        "train_file = '/content/drive/MyDrive/diploma/texts_train.txt'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfkTfwgTPVd4"
      },
      "source": [
        "train_id2tag = {}\n",
        "with codecs.open(train_file, encoding='utf-8') as fin:\n",
        "    for line_no, line in enumerate(fin):\n",
        "        train_id2tag[line.split()[0]] = line_no"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd6J7NZZSbUr"
      },
      "source": [
        "infer_steps = 5\n",
        "infer_alpha = 0.1\n",
        "train_vectors = [dbow.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) \n",
        "                for doc in docs_iterator(train_file, start_from=1)]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tssVLiw8Oy-Y"
      },
      "source": [
        "with codecs.open('/content/drive/MyDrive/diploma/embeddings_doc2vec_train.txt', 'w') as fin:\n",
        "    for id, tag in train_id2tag.items():\n",
        "        line = id + ' ' + ' '.join(map(str, train_vectors[tag])) + '\\n'\n",
        "        fin.write(line)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrM-gS8ZVYBU"
      },
      "source": [
        "test_file = '/content/drive/MyDrive/diploma/texts_test.txt'"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMvLpbdUVYOY"
      },
      "source": [
        "test_id2tag = {}\n",
        "with codecs.open(test_file, encoding='utf-8') as fin:\n",
        "    for line_no, line in enumerate(fin):\n",
        "        test_id2tag[line.split()[0]] = line_no"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJv3mSX3Vc0m"
      },
      "source": [
        "infer_steps = 5\n",
        "infer_alpha = 0.1\n",
        "test_vectors = [dbow.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) \n",
        "                for doc in docs_iterator(test_file, start_from=1)]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y415lchXVjqI"
      },
      "source": [
        "with codecs.open('/content/drive/MyDrive/diploma/embeddings_doc2vec_test.txt', 'w') as fin:\n",
        "    for id, tag in test_id2tag.items():\n",
        "        line = id + ' ' + ' '.join(map(str, test_vectors[tag])) + '\\n'\n",
        "        fin.write(line)"
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}